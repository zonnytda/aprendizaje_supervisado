"""
M√≥dulo: data_preprocessor.py
Proyecto: Predicci√≥n Promedio Final - Egresados UNE
Clase: DataPreprocessor
Responsabilidad: Preprocesar y limpiar datos para Machine Learning
Principio POO: Single Responsibility
Autor: zonny
Fecha: Octubre 2025
"""

import pandas as pd
import numpy as np
import sys
from pathlib import Path
from typing import Tuple, List, Optional
from sklearn.model_selection import train_test_split

# Agregar el directorio padre al path
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

from src.utils.helpers import (
    print_progress,
    print_section_header,
    get_data_quality_report,
    detect_outliers_iqr
)

import warnings
warnings.filterwarnings('ignore')


class DataPreprocessor:
    """
    Preprocesa y limpia datos para Machine Learning.
    
    Caracter√≠sticas:
    - Identificaci√≥n autom√°tica de tipos de variables
    - Manejo inteligente de valores nulos
    - Detecci√≥n y eliminaci√≥n de outliers (IQR/Z-score)
    - Divisi√≥n estratificada train/val/test
    - Conversi√≥n de tipos de datos
    - Generaci√≥n de reportes de calidad
    
    Atributos:
        data (pd.DataFrame): Dataset original
        target_column (str): Nombre de la variable objetivo
        exclude_columns (List[str]): Columnas a excluir del an√°lisis
        numeric_features (List[str]): Lista de features num√©ricas
        categorical_features (List[str]): Lista de features categ√≥ricas
        preprocessed_data (pd.DataFrame): Datos preprocesados
    
    Ejemplo de uso:
        >>> preprocessor = DataPreprocessor(df, target_column='PROMEDIO_FINAL')
        >>> preprocessor.identify_feature_types()
        >>> df_clean = preprocessor.handle_missing_values(strategy='drop')
        >>> df_clean = preprocessor.remove_outliers(method='iqr')
        >>> X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data()
    """
    
    def __init__(self, data: pd.DataFrame, target_column: str, 
                 exclude_columns: Optional[List[str]] = None):
        """
        Inicializa el preprocesador.
        
        Args:
            data: DataFrame con los datos
            target_column: Nombre de la columna objetivo
            exclude_columns: Columnas a excluir del an√°lisis (IDs, fechas, etc.)
        
        Raises:
            ValueError: Si la columna objetivo no existe
        """
        if target_column not in data.columns:
            raise ValueError(f"‚ùå Columna objetivo '{target_column}' no encontrada")
        
        self.data = data.copy()
        self.target_column = target_column
        self.exclude_columns = exclude_columns or []
        
        self.numeric_features: List[str] = []
        self.categorical_features: List[str] = []
        self.preprocessed_data: Optional[pd.DataFrame] = None
        
        # Estad√≠sticas iniciales
        self.initial_shape = self.data.shape
        self.processing_log = []
        
        print(f"‚úÖ DataPreprocessor inicializado")
        print(f"   Dataset: {len(self.data):,} filas x {len(self.data.columns)} columnas")
        print(f"   Target: {self.target_column}")
        print(f"   Columnas excluidas: {len(self.exclude_columns)}")
        
        # Log inicial
        self._add_log("Inicializaci√≥n", f"Shape: {self.data.shape}")
    
    def _add_log(self, action: str, details: str):
        """Agrega entrada al log de procesamiento"""
        self.processing_log.append({
            'action': action,
            'details': details,
            'shape': self.data.shape if self.data is not None else None
        })
    
    def identify_feature_types(self, max_unique_categorical: int = 100) -> Tuple[List[str], List[str]]:
        """
        Identifica autom√°ticamente tipos de variables.
        
        Args:
            max_unique_categorical: M√°ximo de valores √∫nicos para considerar categ√≥rica
        
        Returns:
            Tuple con (features num√©ricas, features categ√≥ricas)
        """
        print_progress("Identificando tipos de variables...", "üîç")
        
        # Columnas a analizar (excluir target y columnas especificadas)
        columns_to_analyze = [
            col for col in self.data.columns 
            if col != self.target_column and col not in self.exclude_columns
        ]
        
        # Identificar num√©ricas
        self.numeric_features = []
        for col in columns_to_analyze:
            if self.data[col].dtype in ['int64', 'float64']:
                # Si tiene muchos valores √∫nicos, es num√©rica continua
                if self.data[col].nunique() > max_unique_categorical:
                    self.numeric_features.append(col)
                else:
                    # Podr√≠a ser categ√≥rica ordinal, pero tratarla como num√©rica
                    self.numeric_features.append(col)
        
        # Identificar categ√≥ricas
        self.categorical_features = [
            col for col in columns_to_analyze
            if col not in self.numeric_features
        ]
        
        print(f"\nüìä An√°lisis de variables:")
        print(f"   {'Variable':<40} {'Tipo':<15} {'√önicos':<10}")
        print("   " + "-"*65)
        
        for col in self.numeric_features:
            print(f"   {col:<40} {'Num√©rica':<15} {self.data[col].nunique():<10}")
        
        for col in self.categorical_features:
            print(f"   {col:<40} {'Categ√≥rica':<15} {self.data[col].nunique():<10}")
        
        print(f"\n‚úÖ Identificaci√≥n completada:")
        print(f"   ‚Ä¢ Variables num√©ricas: {len(self.numeric_features)}")
        print(f"   ‚Ä¢ Variables categ√≥ricas: {len(self.categorical_features)}")
        
        self._add_log("Feature identification", 
                     f"Numeric: {len(self.numeric_features)}, Categorical: {len(self.categorical_features)}")
        
        return self.numeric_features, self.categorical_features
    
    def handle_missing_values(self, strategy: str = 'drop', 
                              threshold: float = 0.5,
                              numeric_strategy: str = 'median',
                              categorical_strategy: str = 'mode') -> pd.DataFrame:
        """
        Maneja valores nulos en el dataset.
        
        Args:
            strategy: Estrategia general ('drop', 'impute', 'smart')
            threshold: % m√°ximo de nulos permitido por columna (0-1)
            numeric_strategy: Para imputaci√≥n num√©rica ('mean', 'median', 'mode')
            categorical_strategy: Para imputaci√≥n categ√≥rica ('mode', 'constant')
        
        Returns:
            DataFrame sin valores nulos
        """
        print_progress(f"Manejando valores nulos (estrategia: {strategy})...", "üîß")
        
        # Analizar valores nulos
        missing = self.data.isnull().sum()
        missing_pct = (missing / len(self.data)) * 100
        
        print(f"\nüìä An√°lisis de valores nulos:")
        has_missing = False
        for col in self.data.columns:
            if missing[col] > 0:
                has_missing = True
                print(f"   ‚Ä¢ {col}: {missing[col]} ({missing_pct[col]:.2f}%)")
        
        if not has_missing:
            print("   ‚úÖ No hay valores nulos!")
            self.preprocessed_data = self.data.copy()
            return self.preprocessed_data
        
        # Aplicar estrategia
        df_clean = self.data.copy()
        rows_before = len(df_clean)
        
        if strategy == 'drop':
            # 1. Eliminar filas con nulos en target
            if df_clean[self.target_column].isnull().any():
                df_clean = df_clean.dropna(subset=[self.target_column])
                print(f"   ‚Ä¢ Eliminadas {rows_before - len(df_clean)} filas (nulos en target)")
            
            # 2. Eliminar columnas con muchos nulos
            cols_to_drop = missing_pct[missing_pct > threshold * 100].index.tolist()
            if self.target_column in cols_to_drop:
                cols_to_drop.remove(self.target_column)
            
            if cols_to_drop:
                df_clean = df_clean.drop(columns=cols_to_drop)
                print(f"   ‚Ä¢ Eliminadas {len(cols_to_drop)} columnas (>{threshold*100}% nulos)")
            
            # 3. Eliminar filas restantes con nulos
            rows_before2 = len(df_clean)
            df_clean = df_clean.dropna()
            if rows_before2 > len(df_clean):
                print(f"   ‚Ä¢ Eliminadas {rows_before2 - len(df_clean)} filas adicionales")
        
        elif strategy == 'impute':
            # Imputar num√©ricas
            for col in self.numeric_features:
                if col in df_clean.columns and df_clean[col].isnull().any():
                    if numeric_strategy == 'mean':
                        value = df_clean[col].mean()
                    elif numeric_strategy == 'median':
                        value = df_clean[col].median()
                    else:  # mode
                        value = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else df_clean[col].mean()
                    
                    df_clean[col].fillna(value, inplace=True)
                    print(f"   ‚Ä¢ {col}: imputado con {numeric_strategy} = {value:.2f}")
            
            # Imputar categ√≥ricas
            for col in self.categorical_features:
                if col in df_clean.columns and df_clean[col].isnull().any():
                    if categorical_strategy == 'mode':
                        mode_value = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'DESCONOCIDO'
                        df_clean[col].fillna(mode_value, inplace=True)
                        print(f"   ‚Ä¢ {col}: imputado con moda = '{mode_value}'")
                    elif categorical_strategy == 'constant':
                        df_clean[col].fillna('DESCONOCIDO', inplace=True)
                        print(f"   ‚Ä¢ {col}: imputado con constante 'DESCONOCIDO'")
        
        elif strategy == 'smart':
            # Estrategia inteligente: combina ambas
            # 1. Eliminar columnas con muchos nulos
            cols_to_drop = missing_pct[missing_pct > threshold * 100].index.tolist()
            if self.target_column in cols_to_drop:
                cols_to_drop.remove(self.target_column)
            if cols_to_drop:
                df_clean = df_clean.drop(columns=cols_to_drop)
                print(f"   ‚Ä¢ Eliminadas {len(cols_to_drop)} columnas (>{threshold*100}% nulos)")
            
            # 2. Imputar nulos restantes
            for col in df_clean.columns:
                if df_clean[col].isnull().any():
                    if col in self.numeric_features:
                        df_clean[col].fillna(df_clean[col].median(), inplace=True)
                    elif col in self.categorical_features:
                        mode = df_clean[col].mode()[0] if len(df_clean[col].mode()) > 0 else 'DESCONOCIDO'
                        df_clean[col].fillna(mode, inplace=True)
        
        print(f"\n‚úÖ Datos limpios:")
        print(f"   Antes: {rows_before:,} filas x {len(self.data.columns)} columnas")
        print(f"   Despu√©s: {len(df_clean):,} filas x {len(df_clean.columns)} columnas")
        print(f"   P√©rdida: {rows_before - len(df_clean):,} filas ({(rows_before - len(df_clean))/rows_before*100:.1f}%)")
        
        self.preprocessed_data = df_clean
        self._add_log("Missing values handled", f"Strategy: {strategy}, Rows lost: {rows_before - len(df_clean)}")
        
        return df_clean
    
    def remove_outliers(self, method: str = 'iqr', 
                       threshold: float = 1.5,
                       columns: Optional[List[str]] = None) -> pd.DataFrame:
        """
        Detecta y elimina outliers en variables num√©ricas.
        
        Args:
            method: M√©todo de detecci√≥n ('iqr' o 'zscore')
            threshold: Umbral (1.5 para IQR, 3 para Z-score)
            columns: Columnas espec√≠ficas a analizar (None = todas las num√©ricas)
        
        Returns:
            DataFrame sin outliers
        """
        print_progress(f"Detectando outliers (m√©todo: {method})...", "üîç")
        
        if self.preprocessed_data is not None:
            df = self.preprocessed_data.copy()
        else:
            df = self.data.copy()
        
        # Determinar columnas a analizar
        cols_to_check = columns if columns else self.numeric_features
        cols_to_check = [col for col in cols_to_check if col in df.columns and col != self.target_column]
        
        if not cols_to_check:
            print("   ‚ö†Ô∏è  No hay columnas num√©ricas para analizar")
            return df
        
        outliers_mask = pd.Series([False] * len(df), index=df.index)
        outliers_by_column = {}
        
        for col in cols_to_check:
            if method == 'iqr':
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - threshold * IQR
                upper_bound = Q3 + threshold * IQR
                
                col_outliers = (df[col] < lower_bound) | (df[col] > upper_bound)
                
            elif method == 'zscore':
                z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
                col_outliers = z_scores > threshold
            
            else:
                raise ValueError(f"M√©todo no v√°lido: {method}")
            
            outliers_count = col_outliers.sum()
            if outliers_count > 0:
                outliers_by_column[col] = outliers_count
                print(f"   ‚Ä¢ {col}: {outliers_count} outliers ({outliers_count/len(df)*100:.1f}%)")
                outliers_mask = outliers_mask | col_outliers
        
        # Eliminar outliers
        before = len(df)
        df_clean = df[~outliers_mask]
        after = len(df_clean)
        
        print(f"\n‚úÖ Outliers eliminados:")
        print(f"   Total de filas con outliers: {before - after} ({(before - after)/before*100:.1f}%)")
        print(f"   Dataset final: {after:,} filas")
        
        self.preprocessed_data = df_clean
        self._add_log("Outliers removed", f"Method: {method}, Rows removed: {before - after}")
        
        return df_clean
    
    def convert_data_types(self) -> pd.DataFrame:
        """
        Convierte tipos de datos apropiadamente.
        
        Returns:
            DataFrame con tipos corregidos
        """
        print_progress("Convirtiendo tipos de datos...", "üîÑ")
        
        if self.preprocessed_data is not None:
            df = self.preprocessed_data.copy()
        else:
            df = self.data.copy()
        
        conversions_made = 0
        
        # Convertir categ√≥ricas a tipo 'category'
        for col in self.categorical_features:
            if col in df.columns:
                if df[col].dtype != 'category':
                    df[col] = df[col].astype('category')
                    conversions_made += 1
                    print(f"   ‚Ä¢ {col}: ‚Üí category ({df[col].nunique()} categor√≠as)")
        
        # Asegurar que num√©ricas sean del tipo correcto
        for col in self.numeric_features:
            if col in df.columns:
                if df[col].dtype == 'object':
                    try:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        conversions_made += 1
                        print(f"   ‚Ä¢ {col}: ‚Üí numeric")
                    except:
                        print(f"   ‚ö†Ô∏è  {col}: no se pudo convertir")
        
        print(f"\n‚úÖ {conversions_made} conversiones realizadas")
        
        self.preprocessed_data = df
        self._add_log("Data types converted", f"Conversions: {conversions_made}")
        
        return df
    
    def split_data(self, test_size: float = 0.15, 
                   val_size: float = 0.15,
                   stratify_column: Optional[str] = None,
                   random_state: int = 42) -> Tuple:
        """
        Divide datos en train, validation y test sets.
        
        Args:
            test_size: Proporci√≥n del test set (0-1)
            val_size: Proporci√≥n del validation set (0-1)
            stratify_column: Columna para estratificaci√≥n (None = sin estratificaci√≥n)
            random_state: Semilla aleatoria para reproducibilidad
        
        Returns:
            Tuple (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        print_progress("Dividiendo datos en train/val/test...", "‚úÇÔ∏è")
        
        if self.preprocessed_data is not None:
            df = self.preprocessed_data.copy()
        else:
            df = self.data.copy()
        
        # Calcular tama√±os
        train_size = 1 - test_size - val_size
        print(f"\nüìä Divisi√≥n:")
        print(f"   ‚Ä¢ Train: {train_size*100:.1f}%")
        print(f"   ‚Ä¢ Validation: {val_size*100:.1f}%")
        print(f"   ‚Ä¢ Test: {test_size*100:.1f}%")
        
        # Separar features y target
        X = df.drop(columns=[self.target_column] + self.exclude_columns, errors='ignore')
        y = df[self.target_column]
        
        # Verificar estratificaci√≥n
        stratify = None
        if stratify_column and stratify_column in df.columns:
            stratify = df[stratify_column]
            print(f"   ‚Ä¢ Estratificaci√≥n por: {stratify_column}")
        
        # Primera divisi√≥n: (train+val) vs test
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y,
            test_size=test_size,
            random_state=random_state,
            stratify=stratify
        )
        
        # Segunda divisi√≥n: train vs val
        val_size_adjusted = val_size / (1 - test_size)
        
        # Estratificaci√≥n para segunda divisi√≥n
        stratify_temp = None
        if stratify_column and stratify_column in X_temp.columns:
            stratify_temp = X_temp[stratify_column]
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp,
            test_size=val_size_adjusted,
            random_state=random_state,
            stratify=stratify_temp
        )
        
        print(f"\n‚úÖ Divisi√≥n completada:")
        print(f"   Train: {len(X_train):,} muestras")
        print(f"   Validation: {len(X_val):,} muestras")
        print(f"   Test: {len(X_test):,} muestras")
        print(f"   Features: {X_train.shape[1]}")
        
        self._add_log("Data split", 
                     f"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def get_preprocessed_data(self) -> pd.DataFrame:
        """
        Obtiene los datos preprocesados.
        
        Returns:
            DataFrame preprocesado (o original si no se ha preprocesado)
        """
        if self.preprocessed_data is None:
            return self.data
        return self.preprocessed_data
    
    def generate_report(self) -> dict:
        """
        Genera reporte completo de preprocesamiento.
        
        Returns:
            Diccionario con estad√≠sticas del preprocesamiento
        """
        current_data = self.preprocessed_data if self.preprocessed_data is not None else self.data
        
        report = {
            'initial_shape': self.initial_shape,
            'final_shape': current_data.shape,
            'rows_lost': self.initial_shape[0] - current_data.shape[0],
            'rows_lost_pct': ((self.initial_shape[0] - current_data.shape[0]) / self.initial_shape[0]) * 100,
            'cols_lost': self.initial_shape[1] - current_data.shape[1],
            'numeric_features': len(self.numeric_features),
            'categorical_features': len(self.categorical_features),
            'target_column': self.target_column,
            'processing_steps': len(self.processing_log),
            'processing_log': self.processing_log,
        }
        
        return report
    
    def print_report(self):
        """Imprime reporte de preprocesamiento formateado"""
        report = self.generate_report()
        
        print_section_header("üìã REPORTE DE PREPROCESAMIENTO")
        
        print(f"üî¢ Datos iniciales: {report['initial_shape'][0]:,} filas x {report['initial_shape'][1]} columnas")
        print(f"üî¢ Datos finales: {report['final_shape'][0]:,} filas x {report['final_shape'][1]} columnas")
        print(f"\nüìä Cambios:")
        print(f"   ‚Ä¢ Filas perdidas: {report['rows_lost']:,} ({report['rows_lost_pct']:.2f}%)")
        print(f"   ‚Ä¢ Columnas perdidas: {report['cols_lost']}")
        
        print(f"\nüéØ Features identificadas:")
        print(f"   ‚Ä¢ Num√©ricas: {report['numeric_features']}")
        print(f"   ‚Ä¢ Categ√≥ricas: {report['categorical_features']}")
        
        print(f"\nüìù Pasos de procesamiento ({report['processing_steps']}):")
        for i, step in enumerate(report['processing_log'], 1):
            print(f"   {i}. {step['action']}: {step['details']}")
        
        print("\n" + "="*70 + "\n")
    
    def __repr__(self) -> str:
        """Representaci√≥n string del objeto"""
        current_data = self.preprocessed_data if self.preprocessed_data is not None else self.data
        return f"DataPreprocessor(rows={len(current_data):,}, target='{self.target_column}')"


# ============================================
# EJEMPLO DE USO
# ============================================

if __name__ == "__main__":
    """
    Ejemplo de uso completo de DataPreprocessor
    """
    from data_loader import DataLoader
    
    print_section_header("üß™ TESTING DATAPREPROCESSOR CLASS")
    
    try:
        # 1. Cargar datos
        print("üìÇ Paso 1: Cargar datos")
        loader = DataLoader('../../data/raw/EGRESADOSUNE20202024.csv')
        df = loader.load_csv()
        
        # 2. Preprocesar
        print("\nüîß Paso 2: Inicializar preprocesador")
        exclude_cols = ['FECHA_CORTE', 'UUID', 'FECHA_EGRESO', 'SITUACION_ALUMNO', 'MODALIDAD']
        
        preprocessor = DataPreprocessor(
            data=df,
            target_column='PROMEDIO_FINAL',
            exclude_columns=exclude_cols
        )
        
        # 3. Pipeline de preprocesamiento
        print("\n‚öôÔ∏è  Paso 3: Pipeline de preprocesamiento")
        preprocessor.identify_feature_types()
        df_clean = preprocessor.handle_missing_values(strategy='drop')
        df_clean = preprocessor.remove_outliers(method='iqr', threshold=1.5)
        df_clean = preprocessor.convert_data_types()
        
        # 4. Split
        print("\n‚úÇÔ∏è  Paso 4: Divisi√≥n de datos")
        X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(
            test_size=0.15,
            val_size=0.15,
            stratify_column='NIVEL_ACADEMICO',
            random_state=42
        )
        
        # 5. Reporte final
        preprocessor.print_report()
        
        print("‚úÖ EJEMPLO COMPLETADO EXITOSAMENTE!")
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()